# H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t Ollama (LLM mi·ªÖn ph√≠, ch·∫°y local)

## üöÄ Ollama l√† g√¨?

Ollama cho ph√©p b·∫°n ch·∫°y LLM (Large Language Models) tr√™n m√°y t√≠nh c·ªßa m√¨nh ho√†n to√†n **MI·ªÑN PH√ç** m√† kh√¥ng c·∫ßn API key.

## üì• C√†i ƒë·∫∑t Ollama

### macOS

```bash
# C√°ch 1: Download t·ª´ website
# Truy c·∫≠p: https://ollama.ai/download
# Download v√† c√†i ƒë·∫∑t file .dmg

# C√°ch 2: S·ª≠ d·ª•ng Homebrew
brew install ollama
```

### Linux

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Windows

Download installer t·ª´: https://ollama.ai/download

## üéØ Kh·ªüi ƒë·ªông Ollama

```bash
# Start Ollama service
ollama serve
```

Ollama s·∫Ω ch·∫°y t·∫°i: `http://localhost:11434`

## üì¶ Download models

### Llama 2 (Khuy√™n d√πng cho ti·∫øng Vi·ªát)

```bash
# Llama 2 7B (4GB RAM)
ollama pull llama2

# Llama 2 13B (8GB RAM) - t·ªët h∆°n nh∆∞ng c·∫ßn nhi·ªÅu RAM
ollama pull llama2:13b
```

### C√°c models kh√°c

```bash
# Mistral 7B - r·∫•t t·ªët
ollama pull mistral

# Qwen2 - t·ªët cho ti·∫øng Trung, ti·∫øng Vi·ªát
ollama pull qwen2

# Gemma - t·ª´ Google
ollama pull gemma:7b

# Phi-3 - nh·ªè g·ªçn, nhanh
ollama pull phi3
```

## ‚öôÔ∏è C·∫•u h√¨nh cho chatbot

### 1. T·∫°o file .env

```bash
cp .env.example .env
```

### 2. Ch·ªânh s·ª≠a .env

```bash
# Ch·ªçn Ollama l√†m provider
LLM_PROVIDER=ollama

# Ch·ªçn model (m·∫∑c ƒë·ªãnh: llama2)
OLLAMA_MODEL=llama2

# URL c·ªßa Ollama service
OLLAMA_BASE_URL=http://localhost:11434
```

### 3. Ch·∫°y chatbot

```bash
python app_gradio.py
```

## üß™ Test Ollama

```bash
# Test xem Ollama c√≥ ho·∫°t ƒë·ªông kh√¥ng
ollama run llama2 "Xin ch√†o, b·∫°n l√† ai?"

# List c√°c models ƒë√£ c√†i
ollama list

# X√≥a model kh√¥ng d√πng
ollama rm mistral
```

## üí° Khuy·∫øn ngh·ªã models

| Model      | RAM c·∫ßn | T·ªëc ƒë·ªô     | Ch·∫•t l∆∞·ª£ng | Ti·∫øng Vi·ªát |
| ---------- | ------- | ---------- | ---------- | ---------- |
| llama2     | 4GB     | ‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê     |
| llama2:13b | 8GB     | ‚≠ê‚≠ê       | ‚≠ê‚≠ê‚≠ê‚≠ê   | ‚≠ê‚≠ê‚≠ê‚≠ê   |
| mistral    | 4GB     | ‚≠ê‚≠ê‚≠ê‚≠ê   | ‚≠ê‚≠ê‚≠ê‚≠ê   | ‚≠ê‚≠ê‚≠ê     |
| qwen2      | 4GB     | ‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê‚≠ê   |
| phi3       | 2GB     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê       |

**Khuy√™n d√πng cho d·ª± √°n n√†y:**

- **llama2**: C√¢n b·∫±ng t·ªët, h·ªó tr·ª£ ti·∫øng Vi·ªát ·ªïn
- **mistral**: Ch·∫•t l∆∞·ª£ng cao, nhanh
- **qwen2**: T·ªët nh·∫•t cho ti·∫øng Vi·ªát v√† ti·∫øng Trung

## üîß Troubleshooting

### L·ªói: "connection refused"

```bash
# Ki·ªÉm tra Ollama c√≥ ch·∫°y kh√¥ng
ps aux | grep ollama

# N·∫øu ch∆∞a ch·∫°y, start l·∫°i
ollama serve
```

### L·ªói: "model not found"

```bash
# Pull model tr∆∞·ªõc
ollama pull llama2
```

### Ch·∫≠m qu√° / H·∫øt RAM

```bash
# D√πng model nh·ªè h∆°n
ollama pull phi3

# Ho·∫∑c d√πng quantized version
ollama pull llama2:7b-q4_0
```

## üéØ So s√°nh v·ªõi c√°c options kh√°c

| Option     | Chi ph√≠     | Setup  | T·ªëc ƒë·ªô     | Ch·∫•t l∆∞·ª£ng |
| ---------- | ----------- | ------ | ---------- | ---------- |
| **Ollama** | ‚úÖ Mi·ªÖn ph√≠ | ‚úÖ D·ªÖ  | ‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê     |
| OpenAI     | ‚ùå Tr·∫£ ph√≠  | ‚úÖ D·ªÖ  | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| vLLM       | ‚úÖ Mi·ªÖn ph√≠ | ‚ùå Kh√≥ | ‚≠ê‚≠ê‚≠ê‚≠ê   | ‚≠ê‚≠ê‚≠ê‚≠ê   |

**‚ú® Ollama l√† l·ª±a ch·ªçn t·ªët nh·∫•t ƒë·ªÉ b·∫Øt ƒë·∫ßu - MI·ªÑN PH√ç v√† D·ªÑ D√ôNG!**

## üìö T√†i li·ªáu tham kh·∫£o

- [Ollama Website](https://ollama.ai/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Model Library](https://ollama.ai/library)
