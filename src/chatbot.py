"""
Chatbot Core - S·ª≠ d·ª•ng Langchain v√† LLM
H·ªó tr·ª£ c·∫£ OpenAI API v√† local LLM (vLLM)
"""

import os
from typing import List, Dict, Optional
from dotenv import load_dotenv

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import Document

from vector_store import VectorStore
from document_processor import DocumentProcessor
from document_compare import DocumentCompare

load_dotenv()


class MEChatbot:
    def __init__(
        self, 
        documents_path: str = "./documents",
        vector_db_path: str = "./vector_db",
        use_local_llm: bool = False
    ):
        self.documents_path = documents_path
        self.vector_db_path = vector_db_path
        
        # Initialize components
        self.document_processor = DocumentProcessor()
        self.vector_store = VectorStore(persist_directory=vector_db_path)
        self.document_compare = DocumentCompare()
        
        # Initialize LLM
        self.llm = self._initialize_llm(use_local_llm)
        
        # Memory for conversation
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # Initialize or load vector store
        self._setup_vector_store()
        
        # Create QA chain
        self.qa_chain = self._create_qa_chain()
    
    def _initialize_llm(self, use_local: bool = False):
        """Initialize LLM - OpenAI ho·∫∑c local vLLM"""
        if use_local:
            # S·ª≠ d·ª•ng local LLM endpoint (vLLM)
            base_url = os.getenv("LLM_API_BASE", "http://localhost:8000/v1")
            model_name = os.getenv("LLM_MODEL_NAME", "Qwen3-14B-AWQ")
            
            return ChatOpenAI(
                model_name=model_name,
                openai_api_base=base_url,
                openai_api_key="EMPTY",  # vLLM kh√¥ng c·∫ßn API key
                temperature=float(os.getenv("TEMPERATURE", 0.7)),
                max_tokens=int(os.getenv("MAX_TOKENS", 2048))
            )
        else:
            # S·ª≠ d·ª•ng OpenAI API
            return ChatOpenAI(
                model_name="gpt-3.5-turbo",
                temperature=float(os.getenv("TEMPERATURE", 0.7)),
                max_tokens=int(os.getenv("MAX_TOKENS", 2048)),
                openai_api_key=os.getenv("OPENAI_API_KEY")
            )
    
    def _setup_vector_store(self):
        """Setup ho·∫∑c load vector store"""
        if not self.vector_store.load():
            print("Creating new vector store from documents...")
            self.rebuild_vector_store()
    
    def rebuild_vector_store(self):
        """Rebuild vector store t·ª´ documents folder"""
        if not os.path.exists(self.documents_path):
            os.makedirs(self.documents_path)
            print(f"Created documents directory: {self.documents_path}")
            print("Please add documents to this directory and run again.")
            return
        
        chunks = self.document_processor.process_directory(self.documents_path)
        
        if chunks:
            self.vector_store.create_vectorstore(chunks)
            self.vector_store.save()
            print(f"Vector store created with {len(chunks)} chunks")
        else:
            print("No documents found to process")
    
    def _create_qa_chain(self):
        """T·∫°o Conversational Retrieval Chain"""
        
        # Custom prompt cho chatbot
        prompt_template = """B·∫°n l√† tr·ª£ l√Ω AI c·ªßa ng√¢n h√†ng ME, h·ªó tr·ª£ 10,000 nh√¢n vi√™n.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu n·ªôi b·ªô ƒë∆∞·ª£c cung c·∫•p.

Ng·ªØ c·∫£nh t·ª´ t√†i li·ªáu:
{context}

L·ªãch s·ª≠ h·ªôi tho·∫°i:
{chat_history}

C√¢u h·ªèi hi·ªán t·∫°i: {question}

H∆∞·ªõng d·∫´n:
1. Tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p
2. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu"
3. Tr√≠ch d·∫´n ngu·ªìn t√†i li·ªáu n·∫øu c√≥ th·ªÉ
4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng v√† chuy√™n nghi·ªáp
5. N·∫øu c·∫ßn so s√°nh t√†i li·ªáu, h√£y ƒë·ªÅ xu·∫•t s·ª≠ d·ª•ng t√≠nh nƒÉng so s√°nh

Tr·∫£ l·ªùi:"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "chat_history", "question"]
        )
        
        # Create chain
        chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vector_store.get_retriever(k=4),
            memory=self.memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": PROMPT},
            verbose=False
        )
        
        return chain
    
    def chat(self, question: str) -> Dict:
        """Chat v·ªõi bot - t√¨m ki·∫øm t√†i li·ªáu v√† tr·∫£ l·ªùi"""
        try:
            if self.qa_chain is None:
                return {
                    "answer": "Vector store ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Vui l√≤ng th√™m t√†i li·ªáu v√†o th∆∞ m·ª•c documents.",
                    "sources": []
                }
            
            result = self.qa_chain({"question": question})
            
            # Format sources
            sources = []
            for doc in result.get("source_documents", []):
                sources.append({
                    "filename": doc.metadata.get("filename", "Unknown"),
                    "content": doc.page_content[:200] + "...",
                    "source": doc.metadata.get("source", "")
                })
            
            return {
                "answer": result["answer"],
                "sources": sources
            }
        
        except Exception as e:
            return {
                "answer": f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}",
                "sources": []
            }
    
    def compare_documents(self, file1: str, file2: str) -> Dict:
        """So s√°nh 2 documents"""
        try:
            return self.document_compare.compare_documents(file1, file2)
        except Exception as e:
            return {
                "error": f"Error comparing documents: {str(e)}"
            }
    
    def search_documents(self, query: str, k: int = 4) -> List[Document]:
        """T√¨m ki·∫øm documents"""
        return self.vector_store.similarity_search(query, k=k)
    
    def add_document(self, file_path: str):
        """Th√™m document m·ªõi v√†o vector store"""
        chunks = self.document_processor.process_document(file_path)
        if chunks:
            self.vector_store.add_documents(chunks)
            self.vector_store.save()
            print(f"Added {len(chunks)} chunks from {file_path}")
    
    def reset_conversation(self):
        """Reset l·ªãch s·ª≠ h·ªôi tho·∫°i"""
        self.memory.clear()


if __name__ == "__main__":
    # Test chatbot
    print("Initializing ME Chatbot...")
    
    # S·ª≠ d·ª•ng OpenAI (default) ho·∫∑c local LLM
    # ƒê·ªïi use_local_llm=True n·∫øu ch·∫°y vLLM local
    chatbot = MEChatbot(use_local_llm=False)
    
    print("\n" + "="*50)
    print("ME Employee Assistant Chatbot")
    print("="*50)
    print("\nCommands:")
    print("  - 'quit' ho·∫∑c 'exit': Tho√°t")
    print("  - 'reset': Reset h·ªôi tho·∫°i")
    print("  - Nh·∫≠p c√¢u h·ªèi ƒë·ªÉ chat")
    print("="*50 + "\n")
    
    while True:
        question = input("B·∫°n: ").strip()
        
        if question.lower() in ['quit', 'exit', 'tho√°t']:
            print("T·∫°m bi·ªát!")
            break
        
        if question.lower() == 'reset':
            chatbot.reset_conversation()
            print("‚úì ƒê√£ reset h·ªôi tho·∫°i\n")
            continue
        
        if not question:
            continue
        
        print("\nƒêang x·ª≠ l√Ω...\n")
        result = chatbot.chat(question)
        
        print(f"Bot: {result['answer']}\n")
        
        if result['sources']:
            print("üìö Ngu·ªìn tham kh·∫£o:")
            for i, source in enumerate(result['sources'][:3], 1):
                print(f"  {i}. {source['filename']}")
            print()
